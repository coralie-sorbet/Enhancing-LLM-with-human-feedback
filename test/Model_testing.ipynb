{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **End-to-End Reinforcement Learning with Human Feedback: Reward Modeling and PPO Testing on unseen texts**"
      ],
      "metadata": {
        "id": "CBOsrdVydIpz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATyYS9GWcvjq",
        "outputId": "43920431-a3d2-4be5-8a1c-7505bdc84a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Load Libraries**\n"
      ],
      "metadata": {
        "id": "-0Z02FCZc6Ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, pipeline"
      ],
      "metadata": {
        "id": "m9pw96M8c-LP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Models on Unseen Texts"
      ],
      "metadata": {
        "id": "VczRbRYUdEB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reward Model"
      ],
      "metadata": {
        "id": "VTD550KCdQ0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained reward model and tokenizer\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(\"drive/MyDrive/M2 D3S/Math of DL/Project/reward_model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"drive/MyDrive/M2 D3S/Math of DL/Project/tokenizer\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "reward_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "\n",
        "# Test inputs and responses\n",
        "prompt = \"What is artificial intelligence?\"\n",
        "responses = [\n",
        "    \"AI is the simulation of human intelligence in machines.\",\n",
        "    \"AI is a field of engineering.\",\n",
        "]\n",
        "\n",
        "# Tokenize and score\n",
        "inputs = tokenizer([prompt] * len(responses), responses, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "# Check for and handle out-of-vocabulary tokens before passing to the model\n",
        "inputs['input_ids'] = inputs['input_ids'].clamp(0, tokenizer.vocab_size - 1)  # Clamp IDs within valid range\n",
        "\n",
        "scores = reward_model(**inputs).logits.squeeze()\n",
        "\n",
        "# Print scores\n",
        "for i, response in enumerate(responses):\n",
        "    print(f\"Response: {response}\\nScore: {scores[i].item()}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVBczFrzdSzh",
        "outputId": "3b126a1b-33e7-45e7-c9ea-42a861d3fc5f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: AI is the simulation of human intelligence in machines.\n",
            "Score: 0.2086963653564453\n",
            "\n",
            "Response: AI is a field of engineering.\n",
            "Score: 0.6473867893218994\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test inputs and responses\n",
        "prompt = \"Explain deep learning.\"\n",
        "responses = [\n",
        "    \"Deep learning uses neural networks to learn patterns.\",\n",
        "    \"It processes data through multiple layers.\"\n",
        "]\n",
        "\n",
        "# Tokenize and score\n",
        "inputs = tokenizer([prompt] * len(responses), responses, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "# Check for and handle out-of-vocabulary tokens before passing to the model\n",
        "inputs['input_ids'] = inputs['input_ids'].clamp(0, tokenizer.vocab_size - 1)  # Clamp IDs within valid range\n",
        "\n",
        "scores = reward_model(**inputs).logits.squeeze()\n",
        "\n",
        "# Print scores\n",
        "for i, response in enumerate(responses):\n",
        "    print(f\"Response: {response}\\nScore: {scores[i].item()}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnR1KwO5_AnW",
        "outputId": "c009bf8e-3cfc-417a-86f9-5ad6eed01d88"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Deep learning uses neural networks to learn patterns.\n",
            "Score: 0.07615280151367188\n",
            "\n",
            "Response: It processes data through multiple layers.\n",
            "Score: -0.652172327041626\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PPO model"
      ],
      "metadata": {
        "id": "Bg2aDmvRdUec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained policy model and tokenizer\n",
        "policy_model = AutoModelForCausalLM.from_pretrained(\"drive/MyDrive/M2 D3S/Math of DL/Project/ppo_optimized\")\n",
        "\n",
        "generation_pipeline = pipeline(\"text-generation\",\n",
        "                               model=policy_model,\n",
        "                               tokenizer=tokenizer,\n",
        "                               device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "diverse_outputs = generation_pipeline(\"What is artificial intelligence?\", num_return_sequences=5)\n",
        "\n",
        "# Calculate and print rewards for diverse_outputs\n",
        "for i, output in enumerate(diverse_outputs):\n",
        "    text = output['generated_text']\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    inputs['input_ids'] = inputs['input_ids'].clamp(0, tokenizer.vocab_size - 1)\n",
        "\n",
        "    score = reward_model(**inputs).logits[0].item()\n",
        "\n",
        "    print(f\"Response: {text}\\nScore: {score}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOYzv4xVdWfK",
        "outputId": "192bf4e6-3f50-4422-c138-4662f6eb5ba6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: What is artificial intelligence?\n",
            "\n",
            "What is the science behind it?\n",
            "\n",
            "I'm sure that there's a lot to learn about it, but I cannot say it definitively yet, and it has to do primarily with a mathematical approach (and I\n",
            "Score: 0.6635737419128418\n",
            "\n",
            "Response: What is artificial intelligence? Why would it exist?\"\n",
            "\n",
            "\"I have met someone before who can tell you that it must do something interesting or interesting and is just like an expert analyst. I'm thinking of him. I know he's not an\n",
            "Score: -2.110313892364502\n",
            "\n",
            "Response: What is artificial intelligence?\n",
            "\n",
            "Curiously smart.\n",
            "\n",
            "This is a far cry from most artificial intelligence technologies.\n",
            "\n",
            "In fact, there are many technologies that I'm not sure of that have made it into the public domain, but\n",
            "Score: -0.4357433319091797\n",
            "\n",
            "Response: What is artificial intelligence? It is very strange when you start with that exact thing that it believes is impossible, that it's impossible, that it exists. If you think about it, I know I'm not making any claims about it. The technology\n",
            "Score: -0.7704994678497314\n",
            "\n",
            "Response: What is artificial intelligence? Can it do things we don't think we should? Will that be its job to say, 'Yes, but we can't give a person anything else'? And if we do that, what can we say?...\n",
            "\n",
            "Score: -0.11132621765136719\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diverse_outputs = generation_pipeline(\"Explain deep learning.\", num_return_sequences=5)\n",
        "\n",
        "# Calculate and print rewards for diverse_outputs\n",
        "for i, output in enumerate(diverse_outputs):\n",
        "    text = output['generated_text']\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    inputs['input_ids'] = inputs['input_ids'].clamp(0, tokenizer.vocab_size - 1)\n",
        "\n",
        "    score = reward_model(**inputs).logits[0].item()\n",
        "\n",
        "    print(f\"Response: {text}\\nScore: {score}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gajUFlg-GS3",
        "outputId": "e54758a0-6eba-44a4-c1ff-2a43155910db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Explain deep learning. The reason you're doing this is we have to understand how we can combine the above code and then learn a new algorithm for what we haven't done before. You probably know this already, but you don't want to think\n",
            "Score: -0.10906124114990234\n",
            "\n",
            "Response: Explain deep learning.\n",
            "\n",
            "It is a bit unclear if the app will work on all Android devices, but there is at least one source app on the App Store that works with either Android devices of your choice.\n",
            "\n",
            "That is probably something\n",
            "Score: -0.7012643814086914\n",
            "\n",
            "Response: Explain deep learning. We don't know if this data was drawn during the project or during any other project — it's just that it took long enough, maybe even tens of hours.\n",
            "\n",
            "What is the potential cost of doing this and,\n",
            "Score: -0.38426971435546875\n",
            "\n",
            "Response: Explain deep learning.\n",
            "\n",
            "It needs good neural networks and algorithms that are not dependent on any algorithm of any algorithm of any algorithm of anything whatsoever. But with a very low price.\n",
            "\n",
            "But we are not really talking about it here,\n",
            "Score: 0.29563069343566895\n",
            "\n",
            "Response: Explain deep learning.\n",
            "\n",
            "Here is a diagram that demonstrates this.\n",
            "\n",
            "In the following image you can see all the variables that we defined earlier (not implemented, but implemented for our app). (Click on any of them to see their\n",
            "Score: -1.3151013851165771\n",
            "\n"
          ]
        }
      ]
    }
  ]
}